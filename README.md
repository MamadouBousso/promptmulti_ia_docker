# PromptingMulti_IA

Un assistant multi-IA intelligent dÃ©veloppÃ© avec Flask et une interface web moderne utilisant Tailwind CSS, intÃ©grant les APIs OpenAI (GPT-4o), Claude (Anthropic) et Groq (Llama) pour des rÃ©ponses intelligentes et comparatives.

## ğŸ“‹ Description

PromptingMulti_IA est une application web qui permet aux utilisateurs d'interagir avec plusieurs assistants IA via un formulaire textuel. L'application supporte trois fournisseurs d'IA majeurs et permet de comparer leurs rÃ©ponses en temps rÃ©el.

## ğŸš€ FonctionnalitÃ©s

- **Interface Web Moderne** : Interface utilisateur responsive avec Tailwind CSS
- **Multi-IA Support** : IntÃ©gration OpenAI (GPT-4o), Claude (Anthropic) et Groq (Llama)
- **Comparaison en Temps RÃ©el** : Compare les rÃ©ponses des trois modÃ¨les simultanÃ©ment
- **Rendu Markdown** : Affichage riche avec support complet du formatage markdown
- **SÃ©lection de ModÃ¨les** : Choix spÃ©cifique des modÃ¨les Llama pour Groq
- **Formulaire Interactif** : Zone de saisie pour les prompts textuels
- **API REST ComplÃ¨te** : Endpoints pour chaque fournisseur d'IA
- **Gestion des Erreurs** : Interface utilisateur avec gestion des erreurs robuste
- **Indicateur de Chargement** : Feedback visuel pendant la gÃ©nÃ©ration
- **Architecture Modulaire** : Structure organisÃ©e avec sÃ©paration des responsabilitÃ©s
- **Environnement Virtuel** : Gestion des dÃ©pendances avec uv

## ğŸ› ï¸ Technologies UtilisÃ©es

- **Backend** : Flask (Python)
- **Frontend** : HTML5, JavaScript, Tailwind CSS
- **IA** : 
  - OpenAI API (GPT-4o)
  - Anthropic API (Claude 3.5 Sonnet)
  - Groq API (Llama 3.1, Llama 4 Scout)
- **Markdown** : marked.js pour le rendu
- **Gestion des DÃ©pendances** : uv
- **Variables d'Environnement** : python-dotenv
- **Architecture** : Pattern MVC (Model-View-Controller)

## ğŸ“ Structure du Projet

```
promptingmulti_ia/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ application/     # Couche application (logique mÃ©tier)
â”‚   â”œâ”€â”€ domaine/         # Couche domaine (entitÃ©s et rÃ¨gles mÃ©tier)
â”‚   â””â”€â”€ infrastructure/  # Couche infrastructure (base de donnÃ©es, API externes)
â”‚       â”œâ”€â”€ openai_client.py   # Client pour l'API OpenAI
â”‚       â”œâ”€â”€ claude_client.py   # Client pour l'API Claude
â”‚       â””â”€â”€ groq_client.py     # Client pour l'API Groq
â”œâ”€â”€ templates/           # Templates HTML Flask
â”‚   â””â”€â”€ index.html       # Page principale de l'application
â”œâ”€â”€ test/               # Tests unitaires et d'intÃ©gration
â”œâ”€â”€ static/             # Fichiers statiques (CSS, JS, images)
â”œâ”€â”€ app.py              # Point d'entrÃ©e de l'application Flask
â”œâ”€â”€ env.example         # Exemple de variables d'environnement
â”œâ”€â”€ pyproject.toml      # Configuration du projet et dÃ©pendances
â””â”€â”€ README.md           # Documentation du projet
```

## ğŸš€ Installation et Configuration

### PrÃ©requis

- Python 3.12+
- uv (gestionnaire de paquets Python)
- ClÃ©s API pour au moins un des fournisseurs d'IA

### Installation

1. **Cloner le repository**
   ```bash
   git clone https://github.com/MamadouBousso/promptingmulti_ia.git
   cd promptingmulti_ia
   ```

2. **Initialiser l'environnement virtuel avec uv**
   ```bash
   uv init
   ```

3. **Installer les dÃ©pendances**
   ```bash
   uv pip install flask openai anthropic groq python-dotenv
   ```

4. **Configurer les variables d'environnement**
   ```bash
   # Copier le fichier d'exemple
   cp env.example .env
   
   # Ã‰diter le fichier .env et ajouter vos clÃ©s API
   # Voir la section Configuration des APIs ci-dessous
   ```

5. **Lancer l'application**
   ```bash
   python app.py
   ```

6. **AccÃ©der Ã  l'application**
   Ouvrez votre navigateur et allez Ã  `http://localhost:8000`

## ğŸ”‘ Configuration des APIs

### Obtenir les clÃ©s API

#### OpenAI (GPT-4o)
1. Allez sur [OpenAI Platform](https://platform.openai.com/)
2. CrÃ©ez un compte ou connectez-vous
3. Allez dans la section "API Keys"
4. CrÃ©ez une nouvelle clÃ© API
5. Copiez la clÃ© et ajoutez-la dans votre fichier `.env`

#### Claude (Anthropic)
1. Allez sur [Anthropic Console](https://console.anthropic.com/)
2. CrÃ©ez un compte ou connectez-vous
3. Allez dans la section "API Keys"
4. CrÃ©ez une nouvelle clÃ© API
5. Copiez la clÃ© et ajoutez-la dans votre fichier `.env`

#### Groq (Llama)
1. Allez sur [Groq Console](https://console.groq.com/)
2. CrÃ©ez un compte ou connectez-vous
3. Allez dans la section "API Keys"
4. CrÃ©ez une nouvelle clÃ© API
5. Copiez la clÃ© et ajoutez-la dans votre fichier `.env`

### Variables d'Environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# Configuration OpenAI (GPT-4o)
OPENAI_API_KEY=sk-your-actual-openai-api-key-here

# Configuration Anthropic (Claude)
ANTHROPIC_API_KEY=sk-ant-your-actual-anthropic-api-key-here

# Configuration Groq (Llama)
GROQ_API_KEY=gsk-your-actual-groq-api-key-here

# Configuration Flask
FLASK_ENV=development
FLASK_DEBUG=True
FLASK_PORT=8000
```

**Note** : Vous n'avez besoin que d'une seule clÃ© API pour commencer. L'application fonctionnera avec les fournisseurs configurÃ©s.

## ğŸ¯ Utilisation

### Interface Utilisateur

L'interface se compose de :

1. **SÃ©lecteur d'IA** : Choisissez entre OpenAI, Claude, Groq ou comparer tous
2. **SÃ©lecteur de ModÃ¨le Groq** : Choisissez le modÃ¨le Llama spÃ©cifique (visible uniquement pour Groq)
3. **Zone de saisie** : Champ de texte pour entrer vos questions/prompts
4. **Bouton d'envoi** : Pour soumettre votre requÃªte
5. **Zone de rÃ©ponse** : Affichage de la rÃ©ponse avec formatage markdown
6. **Zone de comparaison** : Affichage cÃ´te Ã  cÃ´te des rÃ©ponses des trois modÃ¨les
7. **Indicateur de chargement** : Animation pendant la gÃ©nÃ©ration
8. **Zone d'erreur** : Affichage des messages d'erreur

### Fonctionnement

1. **SÃ©lection du fournisseur** : Choisissez OpenAI, Claude, Groq ou "Comparer tous"
2. **SÃ©lection du modÃ¨le** : Si vous choisissez Groq, sÃ©lectionnez le modÃ¨le Llama
3. **Saisie du prompt** : Entrez votre question dans le champ de texte
4. **Envoi** : Cliquez sur "Envoyer" ou appuyez sur EntrÃ©e
5. **Affichage** : La rÃ©ponse s'affiche avec un formatage markdown complet

### ModÃ¨les Disponibles

#### Groq (Llama)
- **Llama 3.1 8B** : ModÃ¨le rapide et efficace
- **Llama 3.1 70B** : ModÃ¨le plus puissant
- **Llama 4 Scout 13B** : ModÃ¨le Ã©quilibrÃ©
- **Llama 4 Scout 17B** : ModÃ¨le performant
- **Llama 4 Scout 32B** : ModÃ¨le haute performance
- **Llama 4 Scout 65B** : ModÃ¨le ultra-performant

## ğŸ“ API Documentation

### Routes Disponibles

#### GET `/`
- **Description** : Page principale avec l'interface utilisateur
- **MÃ©thodes** : GET
- **RÃ©ponse** : Page HTML avec formulaire et interface

#### POST `/api/chat`
- **Description** : API endpoint pour les appels OpenAI
- **MÃ©thodes** : POST
- **Content-Type** : application/json
- **ParamÃ¨tres** :
  ```json
  {
    "prompt": "Votre question ici"
  }
  ```
- **RÃ©ponse** :
  ```json
  {
    "success": true,
    "text": "RÃ©ponse gÃ©nÃ©rÃ©e par OpenAI",
    "prompt": "Question originale"
  }
  ```

#### POST `/api/claude`
- **Description** : API endpoint pour les appels Claude
- **MÃ©thodes** : POST
- **Content-Type** : application/json
- **ParamÃ¨tres** :
  ```json
  {
    "prompt": "Votre question ici"
  }
  ```
- **RÃ©ponse** :
  ```json
  {
    "success": true,
    "text": "RÃ©ponse gÃ©nÃ©rÃ©e par Claude",
    "prompt": "Question originale",
    "model": "claude-3-5-sonnet-20241022",
    "provider": "anthropic"
  }
  ```

#### POST `/api/groq`
- **Description** : API endpoint pour les appels Groq
- **MÃ©thodes** : POST
- **Content-Type** : application/json
- **ParamÃ¨tres** :
  ```json
  {
    "prompt": "Votre question ici",
    "model": "llama3-8b-8192"
  }
  ```
- **RÃ©ponse** :
  ```json
  {
    "success": true,
    "text": "RÃ©ponse gÃ©nÃ©rÃ©e par Groq",
    "prompt": "Question originale",
    "model": "llama3-8b-8192",
    "provider": "groq"
  }
  ```

#### POST `/api/compare`
- **Description** : API endpoint pour comparer les trois fournisseurs
- **MÃ©thodes** : POST
- **Content-Type** : application/json
- **ParamÃ¨tres** :
  ```json
  {
    "prompt": "Votre question ici"
  }
  ```
- **RÃ©ponse** :
  ```json
  {
    "success": true,
    "prompt": "Question originale",
    "responses": {
      "openai": {
        "success": true,
        "text": "RÃ©ponse OpenAI"
      },
      "claude": {
        "success": true,
        "text": "RÃ©ponse Claude"
      },
      "groq": {
        "success": true,
        "text": "RÃ©ponse Groq"
      }
    }
  }
  ```

### Exemples d'utilisation de l'API

#### Test OpenAI
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explique-moi l'intelligence artificielle en 3 points"}'
```

#### Test Claude
```bash
curl -X POST http://localhost:8000/api/claude \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Quels sont les avantages du machine learning ?"}'
```

#### Test Groq
```bash
curl -X POST http://localhost:8000/api/groq \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Comment fonctionne un rÃ©seau de neurones ?", "model": "llama3-8b-8192"}'
```

#### Comparaison des trois
```bash
curl -X POST http://localhost:8000/api/compare \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explique la diffÃ©rence entre l'IA et le machine learning"}'
```

## ğŸ”§ Configuration

### Configuration Flask

L'application Flask est configurÃ©e avec :
- Mode debug activÃ© pour le dÃ©veloppement
- Port par dÃ©faut : 8000 (configurable via FLASK_PORT)
- Templates dans le dossier `templates/`
- Gestion automatique des variables d'environnement

### Configuration des ModÃ¨les

#### OpenAI
- **ModÃ¨le par dÃ©faut** : gpt-4o
- **Max tokens** : 500
- **Temperature** : 0.7
- **Messages systÃ¨me** : Assistant vocal intelligent et utile

#### Claude
- **ModÃ¨le par dÃ©faut** : claude-3-5-sonnet-20241022
- **Max tokens** : 500
- **Temperature** : 0.7
- **Messages systÃ¨me** : Assistant vocal intelligent et utile

#### Groq
- **ModÃ¨le par dÃ©faut** : llama3-8b-8192
- **Max tokens** : 500
- **Temperature** : 0.7
- **Messages systÃ¨me** : Assistant vocal intelligent et utile

## ğŸ¨ Formatage Markdown

L'application supporte le rendu complet du markdown avec :

- **Titres** : `# ## ###` â†’ Titres HTML formatÃ©s
- **Listes** : `- * +` â†’ Listes Ã  puces visibles
- **Listes numÃ©rotÃ©es** : `1. 2.` â†’ Listes ordonnÃ©es
- **Gras** : `**texte**` â†’ Texte en gras
- **Italique** : `*texte*` â†’ Texte en italique
- **Code inline** : `` `code` `` â†’ Code avec fond gris
- **Blocs de code** : ```...``` â†’ Blocs de code formatÃ©s
- **Liens** : `[texte](url)` â†’ Liens cliquables
- **Citations** : `> texte` â†’ Citations avec bordure
- **SÃ©parateurs** : `---` â†’ Lignes de sÃ©paration

## ğŸ§ª Tests

Pour exÃ©cuter les tests (Ã  implÃ©menter) :
```bash
# Tests unitaires
python -m pytest test/

# Tests d'intÃ©gration
python -m pytest test/integration/

# Tests des clients API
python -m pytest test/test_clients/
```

## ğŸ”„ DÃ©veloppement

### Ajout de Nouvelles FonctionnalitÃ©s

1. **Backend** : Ajoutez la logique dans `src/application/`
2. **Infrastructure** : Ajoutez les clients API dans `src/infrastructure/`
3. **Frontend** : Modifiez `templates/index.html`
4. **Tests** : CrÃ©ez les tests correspondants dans `test/`

### Standards de Code

- **Python** : PEP 8
- **JavaScript** : ESLint (Ã  configurer)
- **HTML** : Validation W3C
- **CSS** : Tailwind CSS classes

### Architecture

Le projet suit une architecture en couches :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Templates     â”‚ â† Interface utilisateur
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Application   â”‚ â† Logique mÃ©tier
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Infrastructureâ”‚ â† Clients API externes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ DÃ©ploiement

### Production

1. **Configuration WSGI**
   ```bash
   pip install gunicorn
   gunicorn -w 4 -b 0.0.0.0:8000 app:app
   ```

2. **Variables d'environnement**
   ```bash
   export FLASK_ENV=production
   export FLASK_DEBUG=0
   export OPENAI_API_KEY=your-production-key
   export ANTHROPIC_API_KEY=your-production-key
   export GROQ_API_KEY=your-production-key
   ```

### Docker (Ã  implÃ©menter)

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["python", "app.py"]
```

## ğŸ”’ SÃ©curitÃ©

- **Validation des entrÃ©es** : Tous les prompts sont validÃ©s
- **Gestion des erreurs** : Erreurs API gÃ©rÃ©es gracieusement
- **Variables d'environnement** : ClÃ©s API sÃ©curisÃ©es
- **CORS** : Configuration appropriÃ©e pour la production

## ğŸ“Š Monitoring

- **Logs** : Logs dÃ©taillÃ©s des appels API
- **Erreurs** : Gestion et affichage des erreurs
- **Performance** : Temps de rÃ©ponse des APIs

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©ez une branche feature (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

### Guidelines de Contribution

- Respectez les standards de code
- Ajoutez des tests pour les nouvelles fonctionnalitÃ©s
- Mettez Ã  jour la documentation
- Testez sur plusieurs fournisseurs d'IA

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¥ Auteurs

- **Mamadou Bousso** - DÃ©veloppement initial et maintenance

## ğŸ™ Remerciements

- **Flask** pour le framework web
- **Tailwind CSS** pour les styles
- **OpenAI** pour l'API GPT-4o
- **Anthropic** pour l'API Claude
- **Groq** pour l'API Llama
- **marked.js** pour le rendu markdown
- **uv** pour la gestion des dÃ©pendances

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- Ouvrez une issue sur GitHub
- Contactez l'Ã©quipe de dÃ©veloppement
- Consultez la documentation des APIs

## ğŸ”® Roadmap

- [ ] Support de nouveaux modÃ¨les d'IA
- [ ] Interface de chat en temps rÃ©el
- [ ] Historique des conversations
- [ ] Export des rÃ©ponses
- [ ] Interface d'administration
- [ ] MÃ©triques de performance
- [ ] Support multilingue
- [ ] IntÃ©gration de modÃ¨les locaux

---

**Version** : 2.0.0  
**DerniÃ¨re mise Ã  jour** : 28 Juin 2025  
**Statut** : Production Ready avec support multi-IA
